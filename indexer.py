import os
from dotenv import load_dotenv
import chromadb
from chromadb.utils import embedding_functions

# --- ◊ò◊¢◊ô◊†◊™ ◊û◊©◊™◊†◊ô ◊°◊ë◊ô◊ë◊î ---
env_loaded = load_dotenv()
if env_loaded:
    print("‚úÖ .env file loaded successfully.")
else:
    print("‚ö†Ô∏è .env file not found (this is normal on Render).")

# --- ◊û◊©◊™◊†◊ô◊ù ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHROMA_DIR = os.getenv("CHROMA_DB_DIR", "./data/index")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
MAX_CHUNK_TOKENS = int(os.getenv("MAX_CHUNK_TOKENS", "800"))

# --- ◊ë◊ì◊ô◊ß◊î ◊¢◊ú ◊î◊û◊§◊™◊ó ---
if not OPENAI_API_KEY:
    raise RuntimeError("‚ùå Missing OPENAI_API_KEY ‚Äî add it in your Render environment variables.")

# --- ◊ô◊¶◊ô◊®◊™ ◊™◊ô◊ß◊ô◊ô◊î ◊ú-ChromaDB ---
os.makedirs(CHROMA_DIR, exist_ok=True)

print("üîß Configuration Summary:")
print(f"  ‚Üí CHROMA_DIR: {CHROMA_DIR}")
print(f"  ‚Üí EMBED_MODEL: {EMBED_MODEL}")
print(f"  ‚Üí MAX_CHUNK_TOKENS: {MAX_CHUNK_TOKENS}")

# --- ◊ê◊™◊ó◊ï◊ú ◊ó◊ô◊ë◊ï◊® ◊ú-Chroma ---
client = chromadb.PersistentClient(path=CHROMA_DIR)

# --- ◊§◊ï◊†◊ß◊¶◊ô◊ô◊™ Embedding ---
ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=OPENAI_API_KEY,
    model_name=EMBED_MODEL,
)

# --- ◊ô◊¶◊ô◊®◊™ ◊ê◊ï ◊ò◊¢◊ô◊†◊™ ◊ß◊ï◊ú◊ß◊¶◊ô◊î ---
try:
    collection = client.get_collection(name="shabaton_faq")
    print("‚úÖ Loaded existing collection 'shabaton_faq'")
except Exception:
    collection = client.create_collection(name="shabaton_faq", embedding_function=ef)
    print("üÜï Created new collection 'shabaton_faq'")

# --- ◊ß◊®◊ô◊ê◊™ ◊ß◊ë◊¶◊ô ◊ò◊ß◊°◊ò ---
pages_dir = "data/pages"
if not os.path.exists(pages_dir):
    raise FileNotFoundError(f"‚ùå Directory '{pages_dir}' not found ‚Äî create it and add .txt files.")

files = [f for f in os.listdir(pages_dir) if f.endswith(".txt")]
if not files:
    print("‚ö†Ô∏è No .txt files found in data/pages ‚Äî add content files before indexing.")
    exit(0)
else:
    print(f"üìö Found {len(files)} text files to index.\n")

# --- ◊ê◊ô◊†◊ì◊ï◊ß◊° ◊ß◊ë◊¶◊ô◊ù ---
for fname in files:
    path = os.path.join(pages_dir, fname)
    with open(path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    if not text:
        print(f"‚ö†Ô∏è Skipping empty file: {fname}")
        continue

    # ◊ó◊ú◊ï◊ß◊î ◊ú◊ß◊ò◊¢◊ô◊ù
    max_chars = MAX_CHUNK_TOKENS * 4
    chunks = [
        text[i:i + max_chars]
        for i in range(0, len(text), max_chars)
        if len(text[i:i + max_chars].strip()) > 50
    ]

    ids = [f"{fname}#chunk{i}" for i in range(len(chunks))]
    metas = [
        {
            "source": f"https://www.shabaton.online/{fname.replace('_', '.').replace('.txt', '')}"
        }
        for _ in chunks
    ]

    if chunks:
        try:
            collection.add(documents=chunks, metadatas=metas, ids=ids)
            print(f"[+] Indexed {fname} ({len(chunks)} chunks)")
        except Exception as e:
            print(f"[!] Failed to add {fname}: {e}")
    else:
        print(f"‚ö†Ô∏è No valid text chunks found in {fname}")

print("\n‚úÖ Indexing complete! Your data is ready for querying üöÄ")
