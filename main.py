import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
import openai
import chromadb

# --- ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×” ---
env_loaded = load_dotenv()
if env_loaded:
    print("âœ… .env file loaded successfully.")
else:
    print("âš ï¸ .env file not found (this is normal on Render).")

# --- ××©×ª× ×™ ×§×•× ×¤×™×’×•×¨×¦×™×” ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
TOP_K = int(os.getenv("TOP_K", "4"))
CHROMA_DIR = os.getenv("CHROMA_DB_DIR", "./data/index")
BACKEND_ORIGIN = os.getenv("BACKEND_ORIGIN", "*")

# --- ×‘×“×™×§×” ×¢×œ ×”××¤×ª×— ---
if not OPENAI_API_KEY:
    print("âŒ ERROR: OPENAI_API_KEY missing. Please set it in Render > Environment Variables.")
    raise RuntimeError("OPENAI_API_KEY missing")
else:
    print("âœ… OPENAI_API_KEY loaded successfully.")

# --- ×”×¦×’×ª ×¤×¨×˜×™ ×§×•× ×¤×™×’×•×¨×¦×™×” ---
print("ğŸ”§ Configuration Summary:")
print(f"  â†’ EMBED_MODEL: {EMBED_MODEL}")
print(f"  â†’ LLM_MODEL: {LLM_MODEL}")
print(f"  â†’ CHROMA_DIR: {CHROMA_DIR}")
print(f"  â†’ TOP_K: {TOP_K}")
print(f"  â†’ BACKEND_ORIGIN: {BACKEND_ORIGIN}")

# --- ××ª×—×•×œ OpenAI ---
openai.api_key = OPENAI_API_KEY

# --- ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ-ChromaDB ×× ×œ× ×§×™×™××ª ---
os.makedirs(CHROMA_DIR, exist_ok=True)

# --- ××ª×—×•×œ ×œ×§×•×— Chroma ---
client = chromadb.Client(chromadb.config.Settings(persist_directory=CHROMA_DIR))

# --- ×‘×“×™×§×” ×× ×”×§×•×œ×§×¦×™×” ×§×™×™××ª ---
try:
    collection = client.get_collection("shabaton_faq")
    print("âœ… Chroma collection 'shabaton_faq' loaded successfully.")
except Exception:
    print("âš ï¸ Collection 'shabaton_faq' not found â€” creating a new one...")
    collection = client.create_collection("shabaton_faq")
    print("âœ… New collection 'shabaton_faq' created successfully.")

# --- ××ª×—×•×œ FastAPI ---
app = FastAPI()

# --- CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=[BACKEND_ORIGIN] if BACKEND_ORIGIN != "*" else ["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ××•×“×œ ×‘×§×©×” ---
class QueryIn(BaseModel):
    query: str

# --- ×‘× ×™×™×ª ×¤×¨×•××¤×˜ ---
def build_prompt(question, contexts):
    header = (
        "××ª×” ×¡×•×›×Ÿ ×©×™×¨×•×ª ×¨×©××™ ×¢×‘×•×¨ ××ª×¨ Shabaton.online. "
        "×¢× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“, ×‘×¡×’× ×•×Ÿ ××§×¦×•×¢×™, ××‘×•×¡×¡ ×¢×œ ×”××™×“×¢ ×”× ×ª×•×Ÿ ×‘×œ×‘×“. "
        "×× ××™×Ÿ ××™×“×¢ â€” ×××•×¨ ×©××™×Ÿ ××™×“×¢ ×–××™×Ÿ ×•×”×¤× ×” ×œ×¢××•×“ ×™×¦×™×¨×ª ×§×©×¨.\n\n"
    )
    ctx_texts = []
    for i, c in enumerate(contexts):
        s = f"[××§×•×¨ {i+1}] {c.get('source', '')}\n{c.get('content', '')}\n"
        ctx_texts.append(s)
    return header + "\n\n".join(ctx_texts) + f"\n\n×©××œ×”: {question}\n\n×ª×©×•×‘×” (×‘×¢×‘×¨×™×ª):"

# --- API: ×‘×™×¦×•×¢ ×©××™×œ×ª×” ---
@app.post("/query")
async def query(q: QueryIn):
    qtext = q.query.strip()
    if not qtext:
        raise HTTPException(status_code=400, detail="Empty query")

    # ×©×œ×™×¤×ª ×§×˜×¢×™× ×¨×œ×•×•× ×˜×™×™× ×-Chroma
    try:
        res = collection.query(
            query_texts=[qtext],
            n_results=TOP_K,
            include=["documents", "metadatas"],
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chroma query failed: {e}")

    docs = []
    for doc, meta in zip(res.get("documents", [[]])[0], res.get("metadatas", [[]])[0]):
        docs.append({"content": doc, "source": meta.get("source", "")})

    prompt = build_prompt(qtext, docs)

    # ×§×¨×™××” ×œ-OpenAI
    try:
        completion = openai.ChatCompletion.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "××ª×” ×¢×•×–×¨ ××•×¢×™×œ ×”×¢×•× ×” ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=700,
        )
        answer = completion.choices[0].message.content.strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OpenAI error: {e}")

    sources = list({d["source"] for d in docs if d.get("source")})
    return {"status": "ok", "answer": answer, "sources": sources}

# --- ×©×•×¨×© ×œ×‘×“×™×§×” ---
@app.get("/")
async def root():
    return {"status": "ok", "message": "Shabaton FAQ API is running ğŸš€"}
