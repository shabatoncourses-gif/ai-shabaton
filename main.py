import os
import json
import subprocess
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import chromadb
from openai import OpenAI

# --- ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×” ---
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
CHROMA_DIR = os.getenv("CHROMA_DB_DIR", "./data/index")
SUMMARY_FILE = os.path.join("data", "index_summary.json")

# --- ××¤×œ×™×§×¦×™×” ---
app = FastAPI(title="AI Shabaton API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ×”×ª×—×‘×¨×•×ª ×œ××¡×“ ×”× ×ª×•× ×™× ×©×œ Chroma ---
collection = None
if os.path.exists(CHROMA_DIR):
    try:
        chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)
        collection = chroma_client.get_or_create_collection("shabaton_faq")
        print(f"âœ… Connected to Chroma at {CHROMA_DIR}")
    except Exception as e:
        print(f"âš ï¸ Could not connect to Chroma: {e}")
else:
    print(f"âš ï¸ Chroma directory {CHROMA_DIR} not found.")

# --- × ×§×•×“×ª ×§×¦×” ×œ×©××œ×•×ª ××”×¦'××˜ ---
@app.post("/query")
async def query(request: Request):
    data = await request.json()
    question = data.get("query", "").strip()
    if not question:
        return {"answer": "×œ× ×”×ª×§×‘×œ×” ×©××œ×”.", "sources": []}

    chunks, sources = [], []

    # ğŸ” ×©×œ×‘ 1 â€” ×—×™×¤×•×© ×¨×œ×•×•× ×˜×™ ×‘×××’×¨ Chroma
    if collection:
        try:
            results = collection.query(
                query_texts=[question],
                n_results=4
            )
            chunks = results.get("documents", [[]])[0]
            sources = [
                m.get("source") for m in results.get("metadatas", [[]])[0] if "source" in m
            ]
        except Exception as e:
            print("âš ï¸ Chroma query failed:", e)

    # ğŸ§­ ×©×œ×‘ 2 â€” ×× ××™×Ÿ ×ª×•×¦××•×ª, ×ª×©×•×‘×ª fallback ××•×ª×××ª
    if not chunks:
        print("âš ï¸ No relevant chunks found â€” using fallback message.")
        return {
            "answer": "×œ× × ××¦× ××™×“×¢ ×¨×œ×•×•× ×˜×™, ××•×–×× ×™× ×œ×¤× ×•×ª ×œ×¦×•×•×ª ×©×‘×ª×•×Ÿ ×‘××™×™×œ info@shabaton.co.il",
            "sources": []
        }

    # ğŸ”— ×©×œ×‘ 3 â€” × ×‘× ×” ×”×§×©×¨ ××”×§×˜×¢×™× ×©× ××¦××•
    context = "\n\n---\n\n".join(chunks)
    prompt = (
        "×¢× ×” ×œ×©××œ×” ×”×‘××” ×‘×”×ª×‘×¡×¡ ××š ×•×¨×§ ×¢×œ ×”××™×“×¢ ×”×‘× ××”××ª×¨ ×©×‘×ª×•×Ÿ.\n"
        "×× ××™×Ÿ ×ª×©×•×‘×” ××“×•×™×§×ª, ×›×ª×•×‘:\n"
        "'×œ× × ××¦× ××™×“×¢ ×¨×œ×•×•× ×˜×™, ××•×–×× ×™× ×œ×¤× ×•×ª ×œ×¦×•×•×ª ×©×‘×ª×•×Ÿ ×‘××™×™×œ info@shabaton.co.il'\n\n"
        f"×©××œ×”: {question}\n\n"
        f"××§×˜×¢×™× ×¨×œ×•×•× ×˜×™×™×:\n{context}"
    )

    # ğŸ¤– ×©×œ×‘ 4 â€” ×©×œ×™×—×ª ×”×‘×§×©×” ×œ××•×“×œ
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "××ª×” ×¢×•×–×¨ ××‘×•×¡×¡ ×™×“×¢ ×©×œ ××ª×¨ ×©×‘×ª×•×Ÿ."},
            {"role": "user", "content": prompt}
        ]
    )

    answer = response.choices[0].message.content.strip()

    return {"answer": answer, "sources": list(set(sources))}

# --- ×“×£ ×‘×“×™×§×” ×‘×¡×™×¡×™ ---
@app.get("/")
def root():
    return {
        "status": "ok",
        "message": "Shabaton AI API is running",
        "query_endpoint": "/query"
    }
